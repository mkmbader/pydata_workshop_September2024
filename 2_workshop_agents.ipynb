{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buildling an LLM Agent with LangChain\n",
    "## Notebook 2: Agents\n",
    "---\n",
    "\n",
    "**The goal:** \n",
    "\n",
    "With this notebook you will familiarize yourself with the key concepts of an LLM agent. At the end, you will have all the code you need for your very own agent that uses the tools that you developed in the previous notebook.\n",
    "\n",
    "ðŸŒŸ So ... let us begin!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "1. [Agents](#0)\n",
    "2. [Exercise 1: Explore the agent's output](#1)\n",
    "3. [Exercise 2 : Build your own agent](#2)\n",
    "4. [Exercise 3: Invoke as many tools as you can](#3)\n",
    "5. [Exercise 4 : Optimize the agent prompt](#4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY IF YOU USE GOOGLE COLAB: copy code from helper_functions/colab_utils.txt here and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update helper_functions.keys.py based on private bin link\n",
    "from helper_functions.keys import OPENAI_KEY\n",
    "\n",
    "from helper_functions.tools import *\n",
    "from langchain.agents import create_tool_calling_agent # set up the agent\n",
    "from langchain.agents import AgentExecutor # execute agent\n",
    "from langchain_openai import ChatOpenAI # call openAI as agent llm\n",
    "from langchain import hub\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents <a id='0'></a>\n",
    "\n",
    "Agents combine the functionality of two components: LLMs and Tools. They empower an LLM to be able to execute additional tasks and reason through a problem. Namely, LLMs have knowlegde on data that was used at time of training. However, they lack knowledge about up-to-date happenings and information. They consist of the following components: \n",
    "- **LLM**: A pre-trained LLM.\n",
    "- **List of tools**: List of tools that give additional functionality to the LLM.\n",
    "\n",
    "One use case of LLM Agents is to make it possible to have LLMs with access to real time information, like the current weather. Namely, when a prompt is called, agents have an LLM and Tools at their disposal. If no tool can be found to help in answering the question, the agent tries to answer using the raw LLM. E.g. for a given prompt \"What is the **usual** temperature in Amsterdam in summer?\", an LLM will likely already have knowledge. However, for a prompt \"What is the **current** temperature in Amsterdam?\", a weather API would be a better source of information, and in this case the Agent will decide to use the information from a weather tool. If such a tool is not available to the agent, the agent will respond that the requested information is not available. \n",
    "\n",
    "So, basically, you can think of Agents as usual LLMs but with more \"skills\". Cool, right?\n",
    "\n",
    "Let's see this through an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tools\n",
    "tools = [my_own_wiki_tool, weather_tool, image_tool]\n",
    "\n",
    "# Load LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, api_key=OPENAI_KEY)\n",
    "\n",
    "# With this you let the agent know what its purpose is.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a nice assistant\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Define the agent (load the LLM and the list of tools)\n",
    "agent = create_tool_calling_agent(llm = llm, tools = tools, prompt = prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(\"Your agent is ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: Explore the agent's output <a id='1'></a>\n",
    "\n",
    "**TASK:**\n",
    "In the examples below, read the output to observe how the Agent reasons and decides to use a different tool based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"Where is Amsterdam?\"\n",
    "\n",
    "\n",
    "print(f\"Question 1: {question_1}\")\n",
    "agent_executor.invoke({\"input\": question_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"What is the current temperature in Amsterdam?\"\n",
    "\n",
    "print(f\"Question 1: {question_2}\")\n",
    "agent_executor.invoke({\"input\": question_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_3 = \"What should I visit in Amsterdam? Show me a photo\"\n",
    "\n",
    "print(f\"Question 1: {question_3}\")\n",
    "agent_executor.invoke({\"input\": question_3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2 : Build your own agent  <a id='2'></a>\n",
    "The goal is to build an agent that uses the tools you previously developed. Feel free to also use the pre-made tools, available at `helper_functions/tools.py`\n",
    "\n",
    "**TASK**: \n",
    "- A template for defining an agent and an API key are already provided to you. Build an agent by using a list of tools, and the LLM `gpt-3.5-turbo-0125`.\n",
    "- Test if the agent gives an output.\n",
    "- Observe how the output changes if you provide less tools in your list of tools.\n",
    "- Observe how the output changes if you change the [temperature](https://www.iguazio.com/glossary/llm-temperature/) of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 1 (Tools): Load Tools\n",
    "tools = [\n",
    "# TODO: insert your code here\n",
    "]\n",
    "\n",
    "# Component 2 (LLM): Load LLM\n",
    "llm = ChatOpenAI(model= # TODO: insert your code here           \n",
    "                 temperature=0, \n",
    "                 api_key=OPENAI_KEY)\n",
    "\n",
    "# Component 3 (Prompt): Let the agent know what its purpose is. For now, let's keep it as is.\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages\n",
    "print(type(prompt))\n",
    "\n",
    "# Define the agent and agent executor (load the LLM, the list of tools, and the prompt (descripiton))\n",
    "\n",
    "agent = create_tool_calling_agent(\n",
    "    # TODO: insert your code here\n",
    "    )\n",
    "agent_executor = AgentExecutor(\n",
    "    # TODO: insert your code here\n",
    ")\n",
    "\n",
    "print(\"Your agent is ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Invoke as many tools as you can  <a id='3'></a>\n",
    "\n",
    "**TASK:**\n",
    "Try various questions to call the agent and follow the generated reasoning process in the response. The goal is to call the agent in a way thay it will use as many tools as possible. **Let's see who can reach the highest number of tools used with a single prompt!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "#TODO: Replace this with your question \n",
    "\"\"\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "agent_executor.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 : Optimize the agent prompt <a id='4'></a>\n",
    "So far we played around with the provided LLM and list of tools. Now let's look into the 3rd component: the Agent prompt. \n",
    "\n",
    "**TASK:**\n",
    "- Modify the agent prompt and observe the difference in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 1 (Tools): Load Tools from Exercise 1\n",
    "tools = tools \n",
    "\n",
    "# Component 2 (LLM): Load LLM from Exercise 1\n",
    "llm = llm\n",
    "\n",
    "# Component 3 (Prompt): Create your own prompt to instruct the Agent about its purpose.\n",
    "your_prompt = \"\"\"\"\n",
    "    #  TODO: enter your code here\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", your_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "prompt.messages\n",
    "\n",
    "\n",
    "# This is same as in Exercise 1\n",
    "# Define the agent and agent executor (load the LLM, the list of tools, and the prompt (descripiton))\n",
    "agent = create_tool_calling_agent(\n",
    "    llm = llm, tools = tools, prompt = prompt\n",
    "    )\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Your agent is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe how the same question from before is answered differently with the different prompt.\n",
    "print(f\"Question: {question}\")\n",
    "agent_executor.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒŸ Good job! - You are now ready to proceed to the next (optional) notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
