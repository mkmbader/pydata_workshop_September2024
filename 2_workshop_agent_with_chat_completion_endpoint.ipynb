{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An advanced look at LLM Agents\n",
    "## Notebook 2: Building an agent using the GPT API chat completion endpoint\n",
    "---\n",
    "\n",
    "Previously, we relied on LangChain to to build the agent. However, this is not necessary, since an agent is nothing more than a fancy while loop. \n",
    "\n",
    "**The goal** \n",
    "\n",
    "With this notebook you will see what an agent is under the hood, and you can build it based on the LLM output\n",
    "\n",
    "🌟 So ... let us begin!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "1. [Exercise 1: New tool format](#1)\n",
    "2. [Exercise 2: String vs function call response](#2)\n",
    "3. [Exercise 3: Understanding the agent while-loop](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.path.dirname(os.path.abspath('.'))\n",
    "folder_b_path = os.path.join(current_dir, 'helper_functions')\n",
    "sys.path.append(current_dir)\n",
    "\n",
    "from helper_functions.tools import my_own_wiki_tool, weather_tool\n",
    "from helper_functions.keys import client\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repository\n",
    "To set up Google Colab, follow the steps below. If you would rather run the notebooks locally, visit the `README`\n",
    "\n",
    "1. Open [Google colab](https://colab.research.google.com/) > Open notebook > GitHub \n",
    "2. Paste the [repository link](https://github.com/mkmbader/pydata_workshop_September2024) \n",
    "3. Click on “1_solution_tools_and_agents.ipynb”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS IF YOU USE GOOGLE COLAB. \n",
    "# Otherwise you can COMMENT IT OUT and set up the repository LOCALLY (see instructions in README)\n",
    "\n",
    "!mkdir helper_functions/\n",
    "!mkdir images/\n",
    "!curl https://raw.githubusercontent.com/mkmbader/pydata_workshop_September2024/master/requirements.txt > requirements.txt\n",
    "!curl https://raw.githubusercontent.com/mkmbader/pydata_workshop_September2024/master/helper_functions/helper_functions.py > helper_functions/helper_functions.py\n",
    "!curl https://raw.githubusercontent.com/mkmbader/pydata_workshop_September2024/master/helper_functions/keys.py > helper_functions/keys.py\n",
    "!curl https://raw.githubusercontent.com/mkmbader/pydata_workshop_September2024/master/helper_functions/tools.py > helper_functions/tools.py\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tools\n",
    "\n",
    "When querying OpenAI's API, tools are called via **function calling** (see the API documentation on function calling [here](https://platform.openai.com/docs/assistants/tools/function-calling/quickstart)). Functions have to be passed in JSON format, which we will explore below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: New tool format <a id='1'></a>\n",
    "\n",
    "First, we'll look at an example of how to format the `my_own_wiki_tool` into a function json, then you'll format the `weather_tool` into a function json the same way.\n",
    "\n",
    "**TASK:**  \n",
    "Compile the next two cells and have a look at the example. Do you understand the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: print attributes of my_own_wiki_tool\n",
    "\n",
    "print('name: ', my_own_wiki_tool.name)\n",
    "print('description: ', my_own_wiki_tool.description)\n",
    "print('arguments: ', my_own_wiki_tool.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: json format of the my_own_wiki_tool\n",
    "\n",
    "my_own_wiki_tool_json = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": my_own_wiki_tool.name,\n",
    "        \"description\": my_own_wiki_tool.description,\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            # below all individual function parameters need to be listed\n",
    "            \"properties\": {\n",
    "                'query':{\n",
    "                    'description':'Input search query.',\n",
    "                    'type':'string',\n",
    "                }\n",
    "            },\n",
    "            \"required\": ['query']\n",
    "        },\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:**  \n",
    "Using the example above for guidance, create the json format for the weather tool. If you have trouble filling in the attributes, first print them from the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_tool_json = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": # <TODO: your code here>,\n",
    "        \"description\": # <TODO: your code here>,\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            # below all individual function parameters need to be listed\n",
    "            \"properties\": {\n",
    "                # <TODO: your code here>\n",
    "                }\n",
    "            },\n",
    "            \"required\": # <TODO: your code here>\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the jsonized functions are combined in a list, while the actual tools are stored in a dictionary. After this we have prepared the callable functions and are ready to interact with the chat completions endpoint.\n",
    "\n",
    "**TASK:**  \n",
    "Add the `weather_tool` by completing the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callable functions \n",
    "callable_functions = [my_own_wiki_tool_json, #<TODO: your code here>]\n",
    "\n",
    "# Store executable functions with their name in dictionary\n",
    "available_functions = {\n",
    "    my_own_wiki_tool.name :my_own_wiki_tool, \n",
    "    # <TODO: your code here>\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The LLM response to input questions\n",
    "\n",
    "The LLM can respond with two types of answers to input queries:\n",
    "* a **string** that answers the question, \n",
    "* a **function-call** object, which contains information on which function to call with which arguments.\n",
    "\n",
    "The second one can be used to execute function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: String vs function call response <a id='2'></a>\n",
    "\n",
    "**TASK:**\n",
    "Compile both questions and compare the answers. Do you understand the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a friendly, helpful assistant. Your goal is to answer the questions in a concise, but conversational manner.\"\n",
    "\n",
    "questions = [\"what is the meaning of life?\",\"What temperature is it in Paris?\"]\n",
    "\n",
    "for question in questions:\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "  \n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools = callable_functions,\n",
    "    messages=messages,\n",
    "    )\n",
    "\n",
    "  print(f\"Question: {question}\")\n",
    "  print(f\"Answer: {response.choices[0].message.content}\")\n",
    "  print(f\"Function call: {response.choices[0].message.tool_calls}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK:**  \n",
    "For the function call object above, extract the **name** and the **arguments** of the function call and print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <TODO: Fill in your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Agent - a fancy while loop\n",
    "\n",
    "While the LLM requests function calls we \n",
    "* **extract** the **name and arguments** to be called from the initial LLM response,\n",
    "* **execute** the **function calls**,\n",
    "* **store** the **output of the function** in the messages object,\n",
    "* invoke the LLM again, until no function call are requested.\n",
    "\n",
    "For more details, you can also check out this [OpenAI function calling guide](https://platform.openai.com/docs/guides/function-calling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Understanding the agent while-loop <a id='3'></a>\n",
    "\n",
    "**TASK:**\n",
    "Complete the code below. Then compile the question and investigate the output. Do you understand what you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"which city is bigger: Paris or Munich?\"\n",
    "\n",
    "messages = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  tools = callable_functions,\n",
    "  messages=messages, \n",
    "  tool_choice='required',\n",
    "  )\n",
    "\n",
    "print('==== Initial LLM response ====')\n",
    "print(f\"Answer: {response.choices[0].message.content}\")\n",
    "print(f\"Function call: {response.choices[0].message.tool_calls}\\n\")\n",
    "\n",
    "# while the response requests function calls\n",
    "# TASK: check above how to extract the function call object from the chat completion object. Fill it in below.\n",
    "while # <TODO: fill in your code here>:\n",
    "    \n",
    "  # store response message with all function calls\n",
    "  response_message = response.choices[0].message\n",
    "  messages.append(response_message)\n",
    "\n",
    "  # execute each tool individually\n",
    "  for tool_call in response.choices[0].message.tool_calls:\n",
    "    print('==== Function call ====')\n",
    "\n",
    "    # TASK: EXTRACT FUNCTION NAME AND ARGUMENTS BELOW\n",
    "    # function name and arguments\n",
    "    function_name = #<TODO: fill in your code here>\n",
    "    function_args = json.loads(#<TODO: fill in your code here>)\n",
    "    print(f'Calling function \"{function_name}\" with arguments {function_args}.')\n",
    "\n",
    "    # execute function call \n",
    "    function_response = available_functions[function_name].invoke(function_args)\n",
    "    print(f'Function call response:\\n{function_response}\\n')\n",
    "\n",
    "    # append function response to messages\n",
    "    messages.append({\n",
    "        \"tool_call_id\":tool_call.id, \n",
    "        \"role\": \"tool\", \n",
    "        \"name\": function_name, \n",
    "        \"content\": function_response\n",
    "    })\n",
    "    \n",
    "  # get a new response from LLM\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    tools = callable_functions,\n",
    "    messages=messages, \n",
    "  )\n",
    "\n",
    "  print('==== Intermediate LLM response ====')\n",
    "  print(f\"Answer: {response.choices[0].message.content} \")\n",
    "  print(f\"Function call: {response.choices[0].message.tool_calls }\\n\")\n",
    "\n",
    "print('==== Final LLM response ====')\n",
    "print(\"Question: \", question)\n",
    "print(f\"Answer: {response.choices[0].message.content}\")\n",
    "print(f\"Function call: {response.choices[0].message.tool_calls}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🌟 Congratulations - you've finished the workshop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
