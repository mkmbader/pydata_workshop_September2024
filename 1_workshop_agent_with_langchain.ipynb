{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LLM Agents with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the workshop on building LLM agents with LangChain!\n",
    "    \n",
    "**The use case:** \n",
    "\n",
    "Winter holidays are coming up and you still don't know where to go. Oh no! \n",
    "\n",
    "You decide to build a program that helps you get information on holiday locations. For example, you would like to to find out how big a specific city is, what sights are there to see, how the weather there is, and you would like to get a drawing of that place, to get a first impression. Because who does not like art? \n",
    "\n",
    "You will implement this through an LLM agent, who has access to\n",
    "* the wikipedia API,\n",
    "* a weather API,\n",
    "* can generate images by using a HuggingFace API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚öôÔ∏è Setup\n",
    "\n",
    "Before we start, let's set up the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs\n",
    "\n",
    "\n",
    "#### Weather API \n",
    "We will use the Weather API from `visualcrossing`. You have to generate you API key that you can later use to access this API. Follow the steps below: \n",
    "\n",
    "1. Signup up at https://www.visualcrossing.com/\n",
    "2. Verify your account\n",
    "3. Sign in and click on `Account` (blue button in the top right corner)\n",
    "4. Under `Details` you should be able to see a `Key`\n",
    "5. Copy the Key in `helper_functions/keys.py`\n",
    "\n",
    "\n",
    "#### HuggingFace Token\n",
    "We will use a model avaialble through a HuggingFace API. For that you need to generate a Token. Follow the steps below: \n",
    "\n",
    "1. Visit [HuggingFace](https://huggingface.co/) and sign up or log in.\n",
    "2. Go to your profile (click your avatar), then \"Settings\" > \"Access Tokens.\"\n",
    "3. Click \"New Token,\" select `Fine-grained` as Token Type role, and check the box `Make calls to the serverless Inference API`\n",
    "4. Copy the Token in `helper_functions/keys.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repository\n",
    "To set up Google Colab, follow the steps below. If you would rather run the notebooks locally, visit the `README`\n",
    "\n",
    "1. Open [Google colab](https://colab.research.google.com/) > Open notebook > GitHub \n",
    "2. Paste the [repository link](https://github.com/mkmbader/pydata_workshop_September2024) \n",
    "3. Click on ‚Äú1_workshop_agent_with_langchain.ipynb‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS IF YOU USE GOOGLE COLAB. \n",
    "# Otherwise you can COMMENT IT OUT and set up the repository LOCALLY (see instructions in README)\n",
    "\n",
    "!mkdir helper_functions/\n",
    "!mkdir images/\n",
    "!curl https://raw.githubusercontent.com/mkmbader/pydata_workshop_September2024/master/requirements.txt > requirements.txt\n",
    "!curl https://raw.githubusercontent.com/mkmbader/pydata_workshop_September2024/master/helper_functions/helper_functions.py > helper_functions/helper_functions.py\n",
    "!curl https://raw.githubusercontent.com/mkmbader/pydata_workshop_September2024/master/helper_functions/keys.py > helper_functions/keys.py\n",
    "!curl https://raw.githubusercontent.com/mkmbader/pydata_workshop_September2024/master/helper_functions/tools.py > helper_functions/tools.py\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üåü Nice job, you finished the setup! Now you can deepdive and learn about Agents. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "1. [Default LangChain tools](#1)\n",
    "        <ol type=\"a\">\n",
    "        <li>[Exercise 1 (a): Explore tool parameters](#2)</li>\n",
    "        <li>[Exercise 1 (b): Run tool and explore output](#3)</li>\n",
    "        </ol>\n",
    "2. [Custom tools](#4)\n",
    "        <ol type=\"a\">\n",
    "        <li>[Exercise 2 (a): Build your own Weather tool](#5)</li>\n",
    "        <li>[Exercise 2 (b): Build your own Image tool](#6)</li>\n",
    "        </ol>\n",
    "\n",
    "3. [What are Agents](#7)\n",
    "        <ol type=\"a\">\n",
    "        <li>[Exercise 1: Explore the agent's output](#8)</li>\n",
    "        <li>[Exercise 2 : Build your own agent](#9)</li>\n",
    "        <li>[Exercise 3: Invoke as many tools as you can](#10)</li>\n",
    "        <li>[Exercise 4 : Optimize the agent prompt](#11)</li>\n",
    "        </ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Part 1: Tools\n",
    "---\n",
    "\n",
    "**The goal:** \n",
    "\n",
    "With this part of the notebook you will familiarize yourself with the key concepts of Tools as building blocks of an LLM Agent. At the end, you will have all the code you need to use custom LangChain tools as well as build your own custom tools.\n",
    "\n",
    "üåü So ... let us begin!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reminder:** Make sure to update `helper_functions/keys.py` based on keys in [privatebin](https://privatebin.molops.io/?6ceb2f4c8eabe1d9#HRPAYHTvPraUrdjzU4sHaF6rYA9Snhs23bQxjj2N2cZy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Update helper_functions.keys.py based on private bin link\n",
    "from helper_functions.keys import WEATHER_KEY, HUGGING_FACE_KEY, OPENAI_KEY\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor # execute agent\n",
    "from langchain_openai import ChatOpenAI # call openAI as agent llm\n",
    "from langchain.agents import create_tool_calling_agent # set up the agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Langchain Tools\n",
    "#### Default [Wikipedia tool](https://python.langchain.com/v0.1/docs/integrations/tools/wikipedia/) <a id='1'></a>\n",
    "\n",
    "The cell below loads the full wikipedia tool. It makes an API call to Wikipedia using the ``WikipediaAPIWrapper`` and returns a summary of the queried article. ``WikipediaQueryRun`` then wraps this into a ready made tool. \n",
    "\n",
    "Each tool is a ``BaseTool`` class object, you can find its definition [here](https://api.python.langchain.com/en/latest/tools/langchain_core.tools.BaseTool.html#langchain_core.tools.BaseTool)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper = WikipediaAPIWrapper(top_k_results=1)\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=api_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1 (a): Explore tool parameters <a id='2'></a> [optional]\n",
    "\n",
    "**TASK:**  \n",
    "Use the methods ``name``, ``description``, ``args``, ``return_direct``, ``metadata`` to familiarize yourself with the parameters of the tool. What is the meaning of the different parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Name: \", wiki_tool.name)\n",
    "\n",
    "# TODO: insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.1 (b): Run tool and explore output <a id='3'></a> [optional]\n",
    "\n",
    "**TASK:**\n",
    "* Use the ``.run(tool_input)`` method to execute the tool. The ``tool_input`` is the search term that you'd like to query wikipedia with.\n",
    "* [Optional] Check out the arguments of the WikipediaAPIWrapper [here](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.wikipedia.WikipediaAPIWrapper.html) and modify its parameters above. How does the output change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_input = \"\"\"\n",
    "TODO: insert your code here\n",
    "\"\"\"\n",
    "\n",
    "# Run tool\n",
    "# TODO: insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom tools <a id='4'></a>\n",
    "#### Custom Wikipedia tool\n",
    "\n",
    "You can build your own tools and don't have to rely on default tools. Tools can be built from any function with the LangChain class method ``StructuredTool.from_function()``(see [here](https://python.langchain.com/v0.1/docs/modules/tools/custom_tools/#structuredtool-dataclass)). The basic elements are:\n",
    "* The **function** you would like to be executed when the tool is called\n",
    "* The definition of the **input parameters**\n",
    "* The tool **description**\n",
    "\n",
    "The tool description is especially important, since this is what the agent will use to make the decision if this tool should be used.\n",
    "\n",
    "Below you see the wikipedia tool, built from the basic elements described above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def wikipedia_caller(query:str) ->str:\n",
    "    \"\"\"This function queries wikipedia through a search query.\"\"\"\n",
    "    return api_wrapper.run(query)\n",
    "\n",
    "# Input parameter definition\n",
    "class QueryInput(BaseModel):\n",
    "    query: str = Field(description=\"Input search query\")\n",
    "\n",
    "# the tool description\n",
    "description: str = (\n",
    "        \"A wrapper around Wikipedia. \"\n",
    "        \"Useful for when you need to answer general questions about \"\n",
    "        \"people, places, companies, facts, historical events, or other subjects. \"\n",
    "        \"Input should be a search query.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# fuse the function, input parameters and description into a tool. \n",
    "my_own_wiki_tool = StructuredTool.from_function(\n",
    "    func=wikipedia_caller,\n",
    "    name=\"wikipedia\",\n",
    "    description=description,\n",
    "    args_schema=QueryInput,\n",
    "    return_direct=False,\n",
    ")\n",
    "\n",
    "# test the output of the tool\n",
    "print(my_own_wiki_tool.run('Amsterdam'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1.2 (a): Build your own Weather tool <a id='5'></a>\n",
    "The goal is to build a tool that extracts weather information from the weather site visualcrossing.com. You typically need an API key to extract information from a website. In this example we provide you with the API key. \n",
    "\n",
    "**TASK:** \n",
    "- Build the tool by defining the input parameters and the descriptions. The tool function is already provided to you. \n",
    "- Turn function, description and input parameters into a tool through ``StructuredTool.from_function()``.\n",
    "- Test if the tool gives an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function\n",
    "def extract_city_weather(city:str)->str:\n",
    "\n",
    "    # Build the API URL\n",
    "    url = f\"https://weather.visualcrossing.com/VisualCrossingWebServices/rest/services/timeline/{city}?key={WEATHER_KEY}&unitGroup=metric\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # extract response\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        current_temp = data['days'][0]['temp']\n",
    "        output = f\"Current temperature in {city}: {current_temp}¬∞C\"\n",
    "    else:\n",
    "        output = f\"Error: {response.status_code}\"\n",
    "\n",
    "    return output\n",
    "\n",
    "# Input parameter definition\n",
    "class WeatherInput(BaseModel):\n",
    "    # insert your code here\n",
    "\n",
    "# the tool description\n",
    "description: str = (\n",
    "        # TODO: insert your code here\n",
    "    )\n",
    "\n",
    "# fuse the function, input parameters and description into a tool. \n",
    "my_weather_tool = StructuredTool.from_function(\n",
    "    # TODO: insert your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the output of your Tool\n",
    "print(my_weather_tool.run('Amsterdam'))\n",
    "\n",
    "# TODO: Try generating more ouputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something even more fun. As we previously saw, we can utilize APIs to build tools. Thinking about APIs, one of the biggest collection of models are available via APIs on HuggingFace. So, how about we try to utilize this. \n",
    "\n",
    "#### Exercise 1.2 (b): Build your own Image tool <a id='6'></a>\n",
    "The goal is to build a tool that generates an image based on a given prompt. **That means that later when you can build the Agent you can have an LLM that not only outputs text, but also images!**  \n",
    "\n",
    "To develop this, you can make use of `Stable Diffusion v2-1`, text-to-image model available on HuggingFace. You will use the HuggingFace token that you created in the start. \n",
    "\n",
    "**TASK:** \n",
    "- Build the tool by defining the input parameters and the descriptions. The tool function is already provided to you. \n",
    "- Turn function, description and input parameters into a tool through ``StructuredTool.from_function()``.\n",
    "- Test if the tool gives an output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_image(payload:str):\n",
    "\n",
    "    # Call the text-to-image API with the provided palaod\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/stabilityai/stable-diffusion-2-1\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HUGGING_FACE_KEY}\"}\n",
    "\n",
    "    def query(payload):\n",
    "        response = requests.post(API_URL, headers=headers, json=payload)\n",
    "        return response.content\n",
    "    \n",
    "    image_bytes = query({\n",
    "        \"inputs\": payload,\n",
    "    })\n",
    "\n",
    "    image = Image.open(io.BytesIO(image_bytes))\n",
    "    \n",
    "    # Resize the image\n",
    "    new_size = (400, 400)  # Example new size (width, height)\n",
    "    resized_image = image.resize(new_size)\n",
    "\n",
    "\n",
    "    # Save the resized image to a file\n",
    "    image_path = f'images/image_{payload.replace(\" \", \"_\")}.jpg'\n",
    "    resized_image.save(image_path)\n",
    "    \n",
    "    # Return the path to the saved image\n",
    "    return f'{image_path} '\n",
    "\n",
    "\n",
    "# Input parameter definition\n",
    "class ImageInput(BaseModel):\n",
    "    payload: str = Field(description=\n",
    "        # TODO: insert your code here\n",
    "    )\n",
    "\n",
    "\n",
    "# the tool description\n",
    "images_description: str = (\n",
    "       # TODO: insert your code here\n",
    "    )\n",
    "\n",
    "# fuse the function, input parameters and description into a tool. \n",
    "my_image_tool = StructuredTool.from_function(\n",
    "        # TODO: insert your code here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the output of your Tool\n",
    "print(my_image_tool.run('Amsterdam'))\n",
    "\n",
    "# TODO: Try generating more ouputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you developed an additional tool, make sure to copy your code in `helper_functions/tools.py` in order to later be able to use your tool in an Agent.**\n",
    "\n",
    "Your collection of tools is now ready to be used by an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üåü Done - you are now ready to build an Agent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Part 2: Agents\n",
    "---\n",
    "\n",
    "**The goal:** \n",
    "\n",
    "With this part of the notebook you will familiarize yourself with the key concepts of an LLM agent. At the end, you will have all the code you need for your very own agent that uses the tools that you developed in the previous notebook.\n",
    "\n",
    "üåü So ... let us begin!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Agents <a id='7'></a>\n",
    "\n",
    "Agents combine the functionality of two components: LLMs and Tools. They empower an LLM to be able to execute additional tasks and reason through a problem. Namely, LLMs have knowlegde on data that was used at time of training. However, they lack knowledge about up-to-date happenings and information. They consist of the following components: \n",
    "- **LLM**: A pre-trained LLM.\n",
    "- **List of tools**: List of tools that give additional functionality to the LLM.\n",
    "\n",
    "One use case of LLM Agents is to make it possible to have LLMs with access to real time information, like the current weather. Namely, when a prompt is called, agents have an LLM and Tools at their disposal. If no tool can be found to help in answering the question, the agent tries to answer using the raw LLM. E.g. for a given prompt \"What is the **usual** temperature in Amsterdam in winter?\", an LLM will likely already have knowledge. However, for a prompt \"What is the **current** temperature in Amsterdam?\", a weather API would be a better source of information, and in this case the Agent will decide to use the information from a weather tool. If such a tool is not available to the agent, the agent will respond that the requested information is not available. \n",
    "\n",
    "So, basically, you can think of Agents as usual LLMs but with more \"skills\". Cool, right?\n",
    "\n",
    "Let's see this through an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if you didn't manage to complete all Tools exercises. \n",
    "# You can use the pre-built solutions.\n",
    "# Otherwise comment it out.\n",
    "\n",
    "from helper_functions.tools import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tools\n",
    "tools = [my_own_wiki_tool, weather_tool, image_tool]\n",
    "\n",
    "# Load LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0, api_key=OPENAI_KEY)\n",
    "\n",
    "# With this you let the agent know what its purpose is.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a nice assistant\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Define the agent (load the LLM and the list of tools)\n",
    "agent = create_tool_calling_agent(llm = llm, tools = tools, prompt = prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(\"Your agent is ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.1: Explore the agent's output <a id='8'></a>\n",
    "\n",
    "**TASK:**\n",
    "In the examples below, read the output to observe how the Agent reasons and decides to use a different tool based on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_1 = \"Where is Amsterdam?\"\n",
    "\n",
    "\n",
    "print(f\"Question 1: {question_1}\")\n",
    "agent_executor.invoke({\"input\": question_1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_2 = \"What is the current temperature in Amsterdam?\"\n",
    "\n",
    "print(f\"Question 1: {question_2}\")\n",
    "agent_executor.invoke({\"input\": question_2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_3 = \"What should I visit in Amsterdam? Show me a photo\"\n",
    "\n",
    "print(f\"Question 1: {question_3}\")\n",
    "agent_executor.invoke({\"input\": question_3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.2 : Build your own agent  <a id='9'></a>\n",
    "The goal is to build an agent that uses the tools you previously developed. Feel free to also use the pre-made tools, available at `helper_functions/tools.py`\n",
    "\n",
    "**TASK**: \n",
    "- A template for defining an agent and an API key are already provided to you. Build an agent by using a list of tools, and the LLM `gpt-4o-mini`.\n",
    "- Test if the agent gives an output.\n",
    "- Observe how the output changes if you provide less tools in your list of tools.\n",
    "- Observe how the output changes if you change the [temperature](https://www.iguazio.com/glossary/llm-temperature/) of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 1 (Tools): Load Tools\n",
    "tools = [\n",
    "# TODO: insert your code here\n",
    "]\n",
    "\n",
    "# Component 2 (LLM): Load LLM\n",
    "llm = ChatOpenAI(model= # TODO: insert your code here           \n",
    "                 temperature=0, \n",
    "                 api_key=OPENAI_KEY)\n",
    "\n",
    "# Component 3 (Prompt): Let the agent know what its purpose is. For now, let's keep it as is.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a nice assistant\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "# Define the agent and agent executor (load the LLM, the list of tools, and the prompt (descripiton))\n",
    "agent = create_tool_calling_agent(\n",
    "    # TODO: insert your code here\n",
    "    )\n",
    "agent_executor = AgentExecutor(\n",
    "    # TODO: insert your code here\n",
    ")\n",
    "\n",
    "print(\"Your agent is ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.3: Invoke as many tools as you can  <a id='10'></a>\n",
    "\n",
    "**TASK:**\n",
    "Try various questions to call the agent and follow the generated reasoning process in the response. The goal is to call the agent in a way thay it will use as many tools as possible. **Let's see who can reach the highest number of tools used with a single prompt!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "#TODO: Replace this with your question \n",
    "\"\"\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "agent_executor.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2.4 : Optimize the agent prompt <a id='11'></a>\n",
    "So far we played around with the provided LLM and list of tools. Now let's look into the 3rd component: the Agent prompt. \n",
    "\n",
    "**TASK:**\n",
    "- Modify the agent prompt and observe the difference in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component 1 (Tools): Load Tools from Exercise 1\n",
    "tools = tools \n",
    "\n",
    "# Component 2 (LLM): Load LLM from Exercise 1\n",
    "llm = llm\n",
    "\n",
    "# Component 3 (Prompt): Create your own prompt to instruct the Agent about its purpose.\n",
    "your_prompt = \"\"\"\"\n",
    "    #  TODO: enter your code here\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", your_prompt),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "prompt.messages\n",
    "\n",
    "\n",
    "# This is same as in Exercise 1\n",
    "# Define the agent and agent executor (load the LLM, the list of tools, and the prompt (descripiton))\n",
    "agent = create_tool_calling_agent(\n",
    "    llm = llm, tools = tools, prompt = prompt\n",
    "    )\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Your agent is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe how the same question from before is answered differently with the different prompt.\n",
    "print(f\"Question: {question}\")\n",
    "agent_executor.invoke({\"input\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üåü Good job! - You are now ready to proceed to the next notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
