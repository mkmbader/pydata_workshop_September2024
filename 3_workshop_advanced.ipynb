{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An advanced look at LLM Agents\n",
    "## Notebook 3: Agents behind the hood\n",
    "---\n",
    "\n",
    "Previously, we relied on LangChain to to build the agent. However, this is not necessary, since an agent is nothing more than a fancy while loop. \n",
    "\n",
    "**The goal** \n",
    "\n",
    "With this notebook you will see what an agent is under the hood, and you can build it based on the LLM output.\n",
    "\n",
    "ðŸŒŸ So ... let us begin!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Contents:**\n",
    "\n",
    "1. [Exercise 1: New tool format](#1)\n",
    "2. [Exercise 2: String vs function call response](#2)\n",
    "3. [Exercise 3: Understanding the agent while-loop](#3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY IF YOU USE GOOGLE COLAB: copy code from helper_functions/colab_utils.txt here and compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions.helper_functions import simple_description_formatter\n",
    "from helper_functions.tools import my_own_wiki_tool, weather_tool\n",
    "import pprint\n",
    "from helper_functions.keys import client\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The tools\n",
    "\n",
    "When querying OpneAI's API directly tools are now called functions (see the API documentation on function calling [here](https://platform.openai.com/docs/assistants/tools/function-calling/quickstart)). Functions have to be passed in a specific JSON format, which we will explore below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: New tool format <a id='1'></a>\n",
    "\n",
    "**TASK:**  \n",
    "Compile the cell below and then compare the description of the tool `my_own_wiki_tool` to the formatted function description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the tools and format. In plain OpenAI jargon they are called functions.\n",
    "tools = [my_own_wiki_tool, weather_tool]\n",
    "function_description = [simple_description_formatter(tool) for tool in tools]\n",
    "\n",
    "# Store executable functions with their name in dictionary\n",
    "available_functions = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Description of the my_own_wiki_tool:\\n{ #TODO: your own code here\n",
    "}\\n\")\n",
    "\n",
    "print(\"Formatted function description of my_own_wiki_tool:\")\n",
    "pprint.pprint(function_description[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The LLM response\n",
    "\n",
    "The LLM can respond with two types of answers:\n",
    "* a **string** that answers the question, \n",
    "* a **function-call** object, which contains information on which function to call with which arguments.\n",
    "\n",
    "The second one can be used to execute function calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: String vs function call response <a id='2'></a>\n",
    "\n",
    "**TASK:**\n",
    "Compile both questions and compare the answers. Do you understand the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a friendly, helpful assistant. Your goal is to answer the questions in a concise, but conversational manner.\"\n",
    "\n",
    "questions = [\"what is the meaning of life?\",\"How many people live in Paris?\"]\n",
    "\n",
    "for question in questions:\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "  \n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools = function_description,\n",
    "    messages=messages, \n",
    "    )\n",
    "\n",
    "  print(f\"Question: {question}\")\n",
    "  print(f\"Answer: {response.choices[0].message.content}\")\n",
    "  print(f\"Function call: {response.choices[0].message.tool_calls}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Agent - a fancy while loop\n",
    "\n",
    "While the LLM requests function calls we \n",
    "* **extract** the **name and arguments** to be called from the initial LLM response,\n",
    "* **execute** the **function calls**,\n",
    "* **store** the **output of the function** in the messages object,\n",
    "* invoke the LLM again, until no function call are requested.\n",
    "\n",
    "For more details, you can also check out this [OpenAI function calling guide](https://platform.openai.com/docs/guides/function-calling)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: Understanding the agent while-loop <a id='3'></a>\n",
    "\n",
    "**TASK:**\n",
    "Complete the code below, then compile the question and investigate the output. Do you understand what you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"which city is bigger: Paris or Munich?\"\n",
    "\n",
    "messages = [\n",
    "      {\"role\": \"system\", \"content\": system_prompt},\n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  tools = function_description,\n",
    "  messages=messages, \n",
    "  )\n",
    "\n",
    "print('==== Initial LLM response ====')\n",
    "print(f\"Answer: {#TODO: your code here\n",
    "                 }\")\n",
    "print(f\"Function call: {#TODO: your code here\n",
    "                        }\\n\")\n",
    "\n",
    "# while the response requests function calls\n",
    "while response.choices[0].message.tool_calls:\n",
    "    \n",
    "  # store response message with all function calls\n",
    "  response_message = response.choices[0].message\n",
    "  messages.append(response_message)\n",
    "\n",
    "  # execute each tool individually\n",
    "  for tool_call in response.choices[0].message.tool_calls:\n",
    "    print('==== Function call ====')\n",
    "\n",
    "    # function name and arguments\n",
    "    function_name = tool_call.function.name\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    print(f'Calling function \"{function_name}\" with arguments {function_args}.')\n",
    "\n",
    "    # execute function call \n",
    "    function_response = available_functions[function_name].invoke(function_args)\n",
    "    print(f'Function call response:\\n{function_response}\\n')\n",
    "\n",
    "    # append function response to messages\n",
    "    messages.append({\n",
    "        \"tool_call_id\":tool_call.id, \n",
    "        \"role\": \"tool\", \n",
    "        \"name\": function_name, \n",
    "        \"content\": function_response\n",
    "    })\n",
    "    \n",
    "  # get a new response from LLM\n",
    "  response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    tools = function_description,\n",
    "    messages=messages, \n",
    "  )\n",
    "\n",
    "  print('==== Intermediate LLM response ====')\n",
    "  print(f\"Answer: {#TODO: your code here\n",
    "    \")\n",
    "  print(f\"Function call: {#TODO: your code here\n",
    "                          }\\n\")\n",
    "\n",
    "print('==== Final LLM response ====')\n",
    "print(\"Question: \", question)\n",
    "print(f\"Answer: {response.choices[0].message.content}\")\n",
    "print(f\"Function call: {response.choices[0].message.tool_calls}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŒŸ Congratulations - you've finished the workshop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
